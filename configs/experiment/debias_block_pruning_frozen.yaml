# @package _global_

# A generic config for debiased fine-pruning for various block sizes.
# In this experiment we freeze weights of Bert and we train the scores only.

# Run:
# > python run.py experiment=debias_block_pruning_frozen

# Or to run multiple experiments at once:
# CUDA_VISIBLE_DEVICES=0 python run.py --multirun \
#     experiment=debias_block_pruning_frozen \
#     model.embedding_layer=last,all \
#     model.debias_mode=sentence,token \
#     prune_block_size=32,64,128,256,384,768

defaults:
  - /pruning: sparse_trainer_args.yaml
  - override /trainer: default.yaml
  - override /model: debiaser.yaml
  - override /datamodule: default.yaml
  - override /callbacks: default.yaml
  - override /logger: tensorboard.yaml

# Try: 32,64,128,256,384,768
prune_block_size: 32

# Eg: pruned/B32/bert-base-uncased/all-layer/token-debias
exp_name: pruned/B${prune_block_size}/${model.model_name}/${model.embedding_layer}-layer/${model.debias_mode}-debias

pruning:
  attention_block_rows: ${prune_block_size}
  attention_block_cols: ${prune_block_size}

model:
  _target_: src.models.debiaser_pruned.DebiaserPruned
  model_name: 'bert-base-uncased'
  sparse_train_args: ${pruning}
  freeze_weights: True

datamodule:
  batch_size: 128
  num_workers: 0

callbacks:
  checkpoint_callback:
    compile_pruned: True

trainer:
  max_epochs: 100

