# @package _global_

# A config for debiased attention head fine-pruning (block size 64x768)
# In this experiment we freeze weights of Bert and we train the scores only.

# Run:
# > CUDA_VISIBLE_DEVICES=3 python run.py experiment=debias_head_pruning_mask_lr

defaults:
  - /pruning: sparse_trainer_args.yaml
  - override /trainer: default.yaml
  - override /model: debiaser.yaml
  - override /datamodule: default.yaml
  - override /callbacks: default.yaml
  - override /logger: tensorboard.yaml


pruning:
  attention_block_rows: 64
  attention_block_cols: 768
  mask_scores_learning_rate: 0.001  # 1/10th

model:
  _target_: src.models.debiaser_pruned.DebiaserPruned
  model_name: 'bert-base-uncased'
  sparse_train_args: ${pruning}
  freeze_weights: True

datamodule:
  batch_size: 128
  num_workers: 0

callbacks:
  checkpoint_callback:
    compile_pruned: True

trainer:
  max_epochs: 20

exp_name: pruned/B64x768/mask-lr${pruning.mask_scores_learning_rate}/${model.model_name}/${model.embedding_layer}-layer/${model.debias_mode}-debias
