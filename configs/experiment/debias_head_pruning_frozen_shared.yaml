# @package _global_

# A config for debiased attention head fine-pruning (block size 64x768)
# K, Q, V pruning scores are shared within a layer.
# In this experiment we freeze weights of Bert and we train the scores only.

# Run:
# > python run.py experiment=debias_head_pruning_frozen_shared

# Or to run multiple experiments at once:
# CUDA_VISIBLE_DEVICES=0 python run.py --multirun \
#     experiment=debias_head_pruning_frozen \
#     model.embedding_layer=last,all \
#     model.debias_mode=sentence,token

defaults:
  - /pruning: sparse_trainer_args.yaml
  - override /trainer: default.yaml
  - override /model: debiaser.yaml
  - override /datamodule: default.yaml
  - override /callbacks: default.yaml
  - override /logger: tensorboard.yaml


# Eg: pruned/B64x768/bert-base-uncased/all-layer/token-debias
exp_name: pruned/B64x768-shared/T${pruning.final_threshold}/l1-reg50/${model.model_name}/${model.embedding_layer}-layer/${model.debias_mode}-debias

pruning:
  attention_block_rows: 64
  attention_block_cols: 768
  initial_threshold: 0
  final_threshold: 0.01
  regularization: l1
  regularization_final_lambda: 50.0


model:
  _target_: src.models.debiaser_pruned.DebiaserPruned
  model_name: 'bert-base-uncased'
  sparse_train_args: ${pruning}
  freeze_weights: True
  share_pruning_scores: True

datamodule:
  batch_size: 128
  num_workers: 0

callbacks:
  checkpoint_callback:
    compile_pruned: True

trainer:
  max_epochs: 20

